{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import warnings\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from transformers import logging\n",
    "\n",
    "from medmax.agent import *\n",
    "from medmax.tools import *\n",
    "from medmax.utils import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_error()\n",
    "_ = load_dotenv()\n",
    "\n",
    "PROMPT_FILE = \"medmax/docs/system_prompts.txt\"\n",
    "\n",
    "\"\"\"\n",
    "1. Change the LLM to public LLM\n",
    "3. Explore code generation\n",
    "4. SFT or example database, prompt enhancing to make tool call better\n",
    "5. Add segmentation tool\n",
    "6. How to incorporate new tools easily?\n",
    "7. Implement gradio interface\n",
    "8. New tool: https://www.nature.com/articles/s41551-024-01246-y\n",
    "\n",
    "concrete task with dataset and benchmark + compute\n",
    "mulitmodal still xray report generation is good task to explore -> still unsolved + great potential for commercialization | main problem is hallucination\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_tool = ChestXRayReportGeneratorTool()\n",
    "xray_classification_tool = ChestXRayClassifierTool()\n",
    "medical_visual_qa_tool = MedicalVisualQATool()\n",
    "\n",
    "print(f\"Type: {type(report_tool)}, Name: {report_tool.name}\")\n",
    "print(f\"Type: {type(xray_classification_tool)}, Name: {xray_classification_tool.name}\")\n",
    "print(f\"Type: {type(medical_visual_qa_tool)}, Name: {medical_visual_qa_tool.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = load_prompts_from_file(PROMPT_FILE)\n",
    "prompt = prompts[\"MEDICAL_ASSISTANT\"]\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = MemorySaver()\n",
    "model = ChatOpenAI(model=\"gpt-4o-2024-08-06\", temperature=0, top_p=0.95)\n",
    "agent = Agent(\n",
    "    model, \n",
    "    tools=[xray_classification_tool, medical_visual_qa_tool, report_tool],\n",
    "    log_tools=True,\n",
    "    log_dir=\"logs\",\n",
    "    system_prompt=prompt, \n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Image(agent.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [HumanMessage(content=\"Does the image `demo/chest/pneumonia4.jpg` show pneumonia?\")]\n",
    "# messages = [HumanMessage(content=\"Provide a radiology report for the given image `demo/chest/pneumonia2.jpg`.\")]\n",
    "# messages = [HumanMessage(content=\"Descibe the given image.\")]\n",
    "# messages = [HumanMessage(content=\"What was the first question I asked?\")]\n",
    "# messages = [HumanMessage(content=\"What is the probability of pneumonia in the image `demo/chest/normal1.jpg`?\")]\n",
    "# messages = [HumanMessage(content=\"Does the patient with chest xray given here need to go see doctor? `demo/chest/normal1.jpg`\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for event in agent.workflow.stream({\"messages\": messages}, thread):\n",
    "#     for v in event.values():\n",
    "#         print(v, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7887\n",
      "* Running on public URL: https://f56e08736b7c47aae3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f56e08736b7c47aae3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "import time\n",
    "import shutil\n",
    "from typing import List, Optional\n",
    "from gradio import ChatMessage\n",
    "\n",
    "class ChatInterface:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.upload_dir = Path(\"uploaded_images\")\n",
    "        self.upload_dir.mkdir(exist_ok=True)\n",
    "        self.current_thread_id = None\n",
    "        \n",
    "    def handle_upload(self, image_path: str) -> str:\n",
    "        \"\"\"Handle image upload and return the new path\"\"\"\n",
    "        if not image_path:\n",
    "            return None\n",
    "            \n",
    "        source = Path(image_path)\n",
    "        dest = self.upload_dir / f\"upload_{int(time.time())}{source.suffix}\"\n",
    "        shutil.copy2(source, dest)\n",
    "        return str(dest)\n",
    "\n",
    "    async def process_message(self, \n",
    "                            message: str, \n",
    "                            image: Optional[str], \n",
    "                            chat_history: List[ChatMessage]) -> List[ChatMessage]:\n",
    "        chat_history = chat_history or []\n",
    "        \n",
    "        # Handle image if present\n",
    "        if image:\n",
    "            saved_image = self.handle_upload(image)\n",
    "            message = f\"{message} `{saved_image}`\"\n",
    "        \n",
    "        # Initialize thread ID if needed\n",
    "        if not self.current_thread_id:\n",
    "            self.current_thread_id = str(time.time())\n",
    "        \n",
    "        # Add user message to history\n",
    "        chat_history.append(ChatMessage(role=\"user\", content=message))\n",
    "        yield chat_history\n",
    "        \n",
    "        try:\n",
    "            # Process through your agent's workflow\n",
    "            for event in self.agent.workflow.stream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": message}]},\n",
    "                {\"configurable\": {\"thread_id\": self.current_thread_id}}\n",
    "            ):\n",
    "                if isinstance(event, dict):\n",
    "                    if 'process' in event:\n",
    "                        # Handle main response\n",
    "                        content = event['process']['messages'][-1].content\n",
    "                        if content:\n",
    "                            chat_history.append(ChatMessage(\n",
    "                                role=\"assistant\",\n",
    "                                content=content\n",
    "                            ))\n",
    "                            yield chat_history\n",
    "                    elif 'execute' in event:\n",
    "                        # Handle tool execution\n",
    "                        tool_name = event['execute']['messages'][-1].name\n",
    "                        tool_result = eval(event['execute']['messages'][-1].content)[0]\n",
    "                        if tool_result:\n",
    "                            chat_history.append(ChatMessage(\n",
    "                                role=\"assistant\",\n",
    "                                content=str(tool_result),\n",
    "                                metadata={\"title\": f\"üîß Using {tool_name}\"}\n",
    "                            ))\n",
    "                            yield chat_history\n",
    "                            \n",
    "        except Exception as e:\n",
    "            chat_history.append(ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content=f\"‚ùå Error: {str(e)}\",\n",
    "                metadata={\"title\": \"Error\"}\n",
    "            ))\n",
    "            yield chat_history\n",
    "\n",
    "def create_demo(agent):\n",
    "    interface = ChatInterface(agent)\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üè• Medical X-Ray Analysis Assistant\n",
    "            Upload an X-ray image and ask questions about it.\n",
    "            \"\"\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=3):\n",
    "                    chatbot = gr.Chatbot(\n",
    "                        [],\n",
    "                        height=900,\n",
    "                        container=True,\n",
    "                        show_label=False,\n",
    "                        elem_classes=\"chat-box\",\n",
    "                        type=\"messages\",\n",
    "                        avatar_images=(None, \"üè•\")\n",
    "                    )\n",
    "                    with gr.Row():\n",
    "                        with gr.Column(scale=3):\n",
    "                            txt = gr.Textbox(\n",
    "                                show_label=False,\n",
    "                                placeholder=\"Ask about the X-ray...\",\n",
    "                                container=False\n",
    "                            )\n",
    "                            \n",
    "                \n",
    "                with gr.Column(scale=2):\n",
    "                    image_display = gr.Image(\n",
    "                        label=\"Current X-Ray\",\n",
    "                        type=\"filepath\",\n",
    "                        height=600,\n",
    "                        container=True\n",
    "                    )\n",
    "                    upload_button = gr.UploadButton(\n",
    "                        \"üìé Upload X-Ray\",\n",
    "                        file_types=[\"image\"],\n",
    "                    )\n",
    "                    clear_btn = gr.Button(\"Clear Chat\")\n",
    "                    new_thread_btn = gr.Button(\"New Thread\")\n",
    "\n",
    "        # Event handlers\n",
    "        def clear_chat():\n",
    "            return [], None\n",
    "\n",
    "        def new_thread():\n",
    "            interface.current_thread_id = str(time.time())\n",
    "            return [], None\n",
    "\n",
    "        txt.submit(\n",
    "            interface.process_message,\n",
    "            inputs=[txt, image_display, chatbot],\n",
    "            outputs=chatbot\n",
    "        ).then(\n",
    "            lambda: (\"\", None),\n",
    "            None,\n",
    "            [txt, image_display]\n",
    "        )\n",
    "\n",
    "        upload_button.upload(\n",
    "            lambda x: x,\n",
    "            inputs=upload_button,\n",
    "            outputs=image_display\n",
    "        )\n",
    "\n",
    "        clear_btn.click(clear_chat, outputs=[chatbot, image_display])\n",
    "        new_thread_btn.click(new_thread, outputs=[chatbot, image_display])\n",
    "\n",
    "    return demo\n",
    "\n",
    "demo = create_demo(agent)\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
