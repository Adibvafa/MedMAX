{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. SFT or example database, prompt enhancing to make tool call better\\n2. show segmentation result overlay on the image\\n3. use ppx instead of cm2, support dicom\\n4. look agent benchmark + build benchmark\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "import warnings\n",
    "from typing import *\n",
    "import traceback\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from transformers import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from medmax.agent import *\n",
    "from medmax.tools import *\n",
    "from medmax.utils import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_error()\n",
    "_ = load_dotenv()\n",
    "\n",
    "PROMPT_FILE = \"medmax/docs/system_prompts.txt\"\n",
    "SCRATCH_DIR = \"afallah\"\n",
    "\n",
    "\"\"\"\n",
    "1. SFT or example database, prompt enhancing to make tool call better\n",
    "2. show segmentation result overlay on the image\n",
    "3. use ppx instead of cm2, support dicom\n",
    "4. look agent benchmark + build benchmark\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U git+https://github.com/huggingface/transformers.git@88d960937c81a32bfb63356a2e8ecf7999619681\n",
    "# !pip install -U pydicom gdcm  pylibjpeg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8a97734bfa4254bec3780a6b3e6a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'medmax.tools.report_generation.ChestXRayReportGeneratorTool'>, Name: chest_xray_report_generator\n",
      "Type: <class 'medmax.tools.classification.ChestXRayClassifierTool'>, Name: chest_xray_classifier\n",
      "Type: <class 'medmax.tools.segmentation.ChestXRaySegmentationTool'>, Name: chest_xray_segmentation\n",
      "Type: <class 'medmax.tools.utils.ImageVisualizerTool'>, Name: image_visualizer\n",
      "Type: <class 'medmax.tools.grounding.XRayPhraseGroundingTool'>, Name: xray_phrase_grounding\n",
      "Type: <class 'medmax.tools.dicom.DicomProcessorTool'>, Name: dicom_processor\n"
     ]
    }
   ],
   "source": [
    "report_tool = ChestXRayReportGeneratorTool()\n",
    "xray_classification_tool = ChestXRayClassifierTool()\n",
    "# # # # medical_visual_qa_tool = MedicalVisualQATool()\n",
    "segmentation_tool = ChestXRaySegmentationTool()\n",
    "image_visualizer_tool = ImageVisualizerTool()\n",
    "grounding_tool = XRayPhraseGroundingTool(cache_dir=SCRATCH_DIR, temp_dir=\"temp\")\n",
    "# generation_tool = ChestXRayGeneratorTool(model_path=f\"{SCRATCH_DIR}/roentgen\", temp_dir=\"temp\")\n",
    "dicom_tool = DicomProcessorTool(temp_dir=\"temp\")\n",
    "\n",
    "print(f\"Type: {type(report_tool)}, Name: {report_tool.name}\")\n",
    "print(f\"Type: {type(xray_classification_tool)}, Name: {xray_classification_tool.name}\")\n",
    "# # print(f\"Type: {type(medical_visual_qa_tool)}, Name: {medical_visual_qa_tool.name}\")\n",
    "print(f\"Type: {type(segmentation_tool)}, Name: {segmentation_tool.name}\")\n",
    "print(f\"Type: {type(image_visualizer_tool)}, Name: {image_visualizer_tool.name}\")\n",
    "print(f\"Type: {type(grounding_tool)}, Name: {grounding_tool.name}\")\n",
    "# #print(f\"Type: {type(generation_tool)}, Name: {generation_tool.name}\")\n",
    "print(f\"Type: {type(dicom_tool)}, Name: {dicom_tool.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = load_prompts_from_file(PROMPT_FILE)\n",
    "prompt = prompts[\"MEDICAL_ASSISTANT\"]\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "model = ChatOpenAI(model=\"gpt-4o-2024-08-06\", temperature=0, top_p=0.95)\n",
    "agent = Agent(\n",
    "    model,\n",
    "    tools=[\n",
    "        xray_classification_tool,\n",
    "        report_tool,\n",
    "        segmentation_tool,\n",
    "        image_visualizer_tool,\n",
    "        grounding_tool,\n",
    "        # generation_tool,\n",
    "        #    medical_visual_qa_tool,\n",
    "        dicom_tool\n",
    "    ],\n",
    "    log_tools=True,\n",
    "    log_dir=\"logs\",\n",
    "    system_prompt=prompt,\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Image(agent.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [HumanMessage(content=\"Does the image `demo/chest/pneumonia4.jpg` show pneumonia?\")]\n",
    "# messages = [HumanMessage(content=\"Provide a radiology report for the given image `demo/chest/pneumonia2.jpg`.\")]\n",
    "# messages = [HumanMessage(content=\"Descibe the given image.\")]\n",
    "# messages = [HumanMessage(content=\"What was the first question I asked?\")]\n",
    "# messages = [HumanMessage(content=\"What is the probability of pneumonia in the image `demo/chest/normal1.jpg`?\")]\n",
    "# messages = [HumanMessage(content=\"Does the patient with chest xray given here need to go see doctor? `demo/chest/normal1.jpg`\")]\n",
    "# messages = [HumanMessage(content=\"What is the size of heart?\")]\n",
    "# messages = [HumanMessage(content=\"Classify disease in the image `demo/chest/LIDC.dcm`\")]\n",
    "messages = [HumanMessage(content=\"Segment lungs in `demo/chest/normal1.jpg`.\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rUhcYm7W5HvbNZkDdP6lHTnt', 'function': {'arguments': '{\"image_path\":\"demo/chest/normal1.jpg\",\"organs\":[\"Left Lung\",\"Right Lung\"]}', 'name': 'chest_xray_segmentation'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 1027, 'total_tokens': 1062, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9d50cd990b', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-84983094-cb43-465e-9399-ea004de47da6-0', tool_calls=[{'name': 'chest_xray_segmentation', 'args': {'image_path': 'demo/chest/normal1.jpg', 'organs': ['Left Lung', 'Right Lung']}, 'id': 'call_rUhcYm7W5HvbNZkDdP6lHTnt', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1027, 'output_tokens': 35, 'total_tokens': 1062, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]} \n",
      "\n",
      "Executing tool: {'name': 'chest_xray_segmentation', 'args': {'image_path': 'demo/chest/normal1.jpg', 'organs': ['Left Lung', 'Right Lung']}, 'id': 'call_rUhcYm7W5HvbNZkDdP6lHTnt', 'type': 'tool_call'}\n",
      "Returning to model processing!\n",
      "{'messages': [ToolMessage(content=\"({'segmentation_image_path': 'temp/segmentation_bf58e427.png', 'metrics': {'Left Lung': {'area_pixels': 982877, 'area_cm2': 2457.1925000000006, 'centroid': (923.2938444993625, 1301.34602295099), 'bbox': (0, 980, 1805, 1678), 'width': 698, 'height': 1805, 'aspect_ratio': 2.5859598853868193, 'relative_position': {'top': 0.4566240576159063, 'left': 0.7496232851100172, 'center_dist': 0.2533638822855877}, 'mean_intensity': 103.40283473924, 'std_intensity': 59.90057786407916, 'confidence_score': 0.284039169549942}, 'Right Lung': {'area_pixels': 924676, 'area_cm2': 2311.6900000000005, 'centroid': (878.1622265528682, 543.5882503709407), 'bbox': (47, 102, 1765, 888), 'width': 786, 'height': 1718, 'aspect_ratio': 2.1857506361323153, 'relative_position': {'top': 0.43430377178677954, 'left': 0.3131268723334912, 'center_dist': 0.1980847299649997}, 'mean_intensity': 92.65473419878963, 'std_intensity': 52.9932794616198, 'confidence_score': 0.26804107427597046}}}, {'image_path': 'demo/chest/normal1.jpg', 'segmentation_image_path': 'temp/segmentation_bf58e427.png', 'original_size': (2022, 1736), 'model_size': (512, 512), 'pixel_spacing_mm': 0.5, 'requested_organs': ['Left Lung', 'Right Lung'], 'processed_organs': ['Left Lung', 'Right Lung'], 'analysis_status': 'completed'})\", name='chest_xray_segmentation', tool_call_id='call_rUhcYm7W5HvbNZkDdP6lHTnt', args={'image_path': 'demo/chest/normal1.jpg', 'organs': ['Left Lung', 'Right Lung']})]} \n",
      "\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rMb3H5uQkk4bCYk8f3Mb3zfM', 'function': {'arguments': '{\"image_path\":\"temp/segmentation_bf58e427.png\",\"title\":\"Lung Segmentation\",\"description\":\"Segmented areas for the left and right lungs in the chest X-ray image.\"}', 'name': 'image_visualizer'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 1547, 'total_tokens': 1598, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1024}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9d50cd990b', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e4e819f5-415e-4286-8e09-4606d278be4f-0', tool_calls=[{'name': 'image_visualizer', 'args': {'image_path': 'temp/segmentation_bf58e427.png', 'title': 'Lung Segmentation', 'description': 'Segmented areas for the left and right lungs in the chest X-ray image.'}, 'id': 'call_rMb3H5uQkk4bCYk8f3Mb3zfM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1547, 'output_tokens': 51, 'total_tokens': 1598, 'input_token_details': {'audio': 0, 'cache_read': 1024}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]} \n",
      "\n",
      "Executing tool: {'name': 'image_visualizer', 'args': {'image_path': 'temp/segmentation_bf58e427.png', 'title': 'Lung Segmentation', 'description': 'Segmented areas for the left and right lungs in the chest X-ray image.'}, 'id': 'call_rMb3H5uQkk4bCYk8f3Mb3zfM', 'type': 'tool_call'}\n",
      "Returning to model processing!\n",
      "{'messages': [ToolMessage(content=\"({'image_path': 'temp/segmentation_bf58e427.png'}, {'image_path': 'temp/segmentation_bf58e427.png', 'title': True, 'description': True, 'figsize': (10, 10), 'cmap': 'rgb', 'analysis_status': 'completed'})\", name='image_visualizer', tool_call_id='call_rMb3H5uQkk4bCYk8f3Mb3zfM', args={'image_path': 'temp/segmentation_bf58e427.png', 'title': 'Lung Segmentation', 'description': 'Segmented areas for the left and right lungs in the chest X-ray image.'})]} \n",
      "\n",
      "{'messages': [AIMessage(content='Here is the segmented image showing the left and right lungs in the chest X-ray:\\n\\n![Lung Segmentation](temp/segmentation_bf58e427.png)\\n\\nThe segmented areas highlight the left and right lungs in the image. If you need further details or analysis, feel free to ask!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 1673, 'total_tokens': 1734, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1536}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9d50cd990b', 'finish_reason': 'stop', 'logprobs': None}, id='run-ae7b4893-cecd-46ed-b55c-18bc2492dddf-0', usage_metadata={'input_tokens': 1673, 'output_tokens': 61, 'total_tokens': 1734, 'input_token_details': {'audio': 0, 'cache_read': 1536}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for event in agent.workflow.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool: {'name': 'chest_xray_segmentation', 'args': {'image_path': '/tmp/gradio/ea8bb835dbb397ee5bb1afdc7f8fbed04c20b22fc548154571b6163d116017fe/normal1.jpeg', 'organs': ['Left Lung', 'Right Lung']}, 'id': 'call_lZJPvhbCjzjxdHcJz8IjXnLT', 'type': 'tool_call'}\n",
      "Returning to model processing!\n",
      "Executing tool: {'name': 'image_visualizer', 'args': {'image_path': 'temp/segmentation_bd133dc3.png', 'title': 'Lung Segmentation', 'description': 'Segmented areas for the left and right lungs are highlighted in the image.'}, 'id': 'call_Hliotb2Fi3rwC0H4jqO9M4uA', 'type': 'tool_call'}\n",
      "Returning to model processing!\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "import time\n",
    "import shutil\n",
    "from typing import List, Optional, Tuple\n",
    "from gradio import ChatMessage\n",
    "\n",
    "class ChatInterface:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.upload_dir = Path(\"temp\")\n",
    "        self.upload_dir.mkdir(exist_ok=True)\n",
    "        self.current_thread_id = None\n",
    "        # Separate storage for original and display paths\n",
    "        self.original_file_path = None  # For LLM (.dcm or other)\n",
    "        self.display_file_path = None   # For UI (always viewable format)\n",
    "\n",
    "    def handle_upload(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Handle new file upload and set appropriate paths\n",
    "        Returns: display_path for UI\n",
    "        \"\"\"\n",
    "        if not file_path:\n",
    "            return None\n",
    "            \n",
    "        source = Path(file_path)\n",
    "        timestamp = int(time.time())\n",
    "        \n",
    "        # Save original file with proper suffix\n",
    "        suffix = source.suffix.lower()\n",
    "        saved_path = self.upload_dir / f\"upload_{timestamp}{suffix}\"\n",
    "        shutil.copy2(file_path, saved_path)  # Use file_path directly instead of source\n",
    "        self.original_file_path = str(saved_path)\n",
    "        \n",
    "        # Handle DICOM conversion for display only\n",
    "        if suffix == '.dcm':\n",
    "            output, _ = dicom_tool._run(str(saved_path))\n",
    "            self.display_file_path = output['image_path']\n",
    "        else:\n",
    "            self.display_file_path = str(saved_path)\n",
    "        \n",
    "        return self.display_file_path\n",
    "\n",
    "    async def process_message(self, \n",
    "                        message: str, \n",
    "                        display_image: Optional[str], \n",
    "                        chat_history: List[ChatMessage]) -> List[ChatMessage]:\n",
    "        chat_history = chat_history or []\n",
    "        \n",
    "        # Use original file path if available, otherwise use display image path\n",
    "        image_path = self.original_file_path or display_image\n",
    "        if image_path:\n",
    "            message = f\"{message} `{image_path}`\"\n",
    "        \n",
    "        # Use original file path for LLM prompt\n",
    "        if self.original_file_path:\n",
    "            message = f\"{message} `{self.original_file_path}`\"\n",
    "        \n",
    "        # Initialize thread if needed\n",
    "        if not self.current_thread_id:\n",
    "            self.current_thread_id = str(time.time())\n",
    "        \n",
    "        chat_history.append(ChatMessage(role=\"user\", content=message))\n",
    "        yield chat_history, self.display_file_path\n",
    "        \n",
    "        try:\n",
    "            for event in self.agent.workflow.stream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": message}]},\n",
    "                {\"configurable\": {\"thread_id\": self.current_thread_id}}\n",
    "            ):\n",
    "                if isinstance(event, dict):\n",
    "                    if 'process' in event:\n",
    "                        content = event['process']['messages'][-1].content\n",
    "                        if content:\n",
    "                            chat_history.append(ChatMessage(\n",
    "                                role=\"assistant\",\n",
    "                                content=content\n",
    "                            ))\n",
    "                            yield chat_history, self.display_file_path\n",
    "\n",
    "                    elif 'execute' in event:\n",
    "                        for message in event['execute']['messages']:\n",
    "                            tool_name = message.name\n",
    "                            tool_result = eval(message.content)[0]\n",
    "                            \n",
    "                            # For image_visualizer, use display path\n",
    "                            if tool_name == \"image_visualizer\":\n",
    "                                self.display_file_path = tool_result['image_path']\n",
    "                            \n",
    "                            if tool_result:\n",
    "                                formatted_result = ' '.join(line.strip() for line in str(tool_result).splitlines()).strip()\n",
    "                                chat_history.append(ChatMessage(\n",
    "                                    role=\"assistant\",\n",
    "                                    content=formatted_result,\n",
    "                                    metadata={\"title\": f\"üîß Using tool: {tool_name}\"},\n",
    "                                ))\n",
    "                                yield chat_history, self.display_file_path\n",
    "                            \n",
    "        except Exception as e:\n",
    "            chat_history.append(ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content=f\"‚ùå Error: {str(e)}\",\n",
    "                metadata={\"title\": \"Error\"}\n",
    "            ))\n",
    "            yield chat_history, self.display_file_path\n",
    "\n",
    "def create_demo(agent):\n",
    "    interface = ChatInterface(agent)\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üè• MedMAX\n",
    "            Multimodal Medical Agent for Chest X-rays\n",
    "            \"\"\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=3):\n",
    "                    chatbot = gr.Chatbot(\n",
    "                        [],\n",
    "                        height=800,\n",
    "                        container=True,\n",
    "                        show_label=True,\n",
    "                        elem_classes=\"chat-box\",\n",
    "                        type=\"messages\",\n",
    "                        label=\"Agent\",\n",
    "                        avatar_images=(None, \"https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png\")\n",
    "                    )\n",
    "                    with gr.Row():\n",
    "                        with gr.Column(scale=3):\n",
    "                            txt = gr.Textbox(\n",
    "                                show_label=False,\n",
    "                                placeholder=\"Ask about the X-ray...\",\n",
    "                                container=False\n",
    "                            )\n",
    "                            \n",
    "                with gr.Column(scale=3):\n",
    "                    image_display = gr.Image(\n",
    "                        label=\"Image\",\n",
    "                        type=\"filepath\",\n",
    "                        height=700,\n",
    "                        container=True\n",
    "                    )\n",
    "                    with gr.Row():\n",
    "                        upload_button = gr.UploadButton(\n",
    "                            \"üìé Upload X-Ray\",\n",
    "                            file_types=[\"image\"],\n",
    "                        )\n",
    "                        dicom_upload = gr.UploadButton(\n",
    "                            \"üìÑ Upload DICOM\",\n",
    "                            file_types=[\"file\"],\n",
    "                        )\n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\"Clear Chat\")\n",
    "                        new_thread_btn = gr.Button(\"New Thread\")\n",
    "\n",
    "        # Event handlers\n",
    "        def clear_chat():\n",
    "            interface.original_file_path = None\n",
    "            interface.display_file_path = None\n",
    "            return [], None\n",
    "\n",
    "        def new_thread():\n",
    "            interface.current_thread_id = str(time.time())\n",
    "            return [], interface.display_file_path\n",
    "\n",
    "        def handle_file_upload(file):\n",
    "            return interface.handle_upload(file.name)\n",
    "\n",
    "        txt.submit(\n",
    "            interface.process_message,\n",
    "            inputs=[txt, image_display, chatbot],\n",
    "            outputs=[chatbot, image_display]\n",
    "        ).then(\n",
    "            lambda: \"\",\n",
    "            None,\n",
    "            [txt]\n",
    "        )\n",
    "\n",
    "        upload_button.upload(\n",
    "            handle_file_upload,\n",
    "            inputs=upload_button,\n",
    "            outputs=image_display\n",
    "        )\n",
    "        \n",
    "        dicom_upload.upload(\n",
    "            handle_file_upload,\n",
    "            inputs=dicom_upload,\n",
    "            outputs=image_display\n",
    "        )\n",
    "\n",
    "        clear_btn.click(clear_chat, outputs=[chatbot, image_display])\n",
    "        new_thread_btn.click(new_thread, outputs=[chatbot, image_display])\n",
    "\n",
    "    return demo\n",
    "\n",
    "demo = create_demo(agent)\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medmax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
