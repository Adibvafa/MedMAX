{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import warnings\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from transformers import logging\n",
    "\n",
    "from medixar.agent import *\n",
    "from medixar.tools import *\n",
    "from medixar.utils import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_error()\n",
    "_ = load_dotenv()\n",
    "\n",
    "PROMPT_FILE = \"medixar/docs/system_prompts.txt\"\n",
    "\n",
    "\"\"\"\n",
    "1. Change the LLM to public LLM\n",
    "2. Add other tools + integrate tool baackend\n",
    "4. Choose agent name\n",
    "5. Explore code generation\n",
    "6. SFT or example database, prompt enhancing to make tool call better\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_tool = RadiologyReportGeneratorTool()\n",
    "organ_size_tool = OrganSizeMeasurementTool()\n",
    "chest_xray_tool = ChestXRayClassifierTool()\n",
    "\n",
    "print(f\"Type: {type(report_tool)}, Name: {report_tool.name}\")\n",
    "print(f\"Type: {type(organ_size_tool)}, Name: {organ_size_tool.name}\")\n",
    "print(f\"Type: {type(chest_xray_tool)}, Name: {chest_xray_tool.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = load_prompts_from_file(PROMPT_FILE)\n",
    "prompt = prompts[\"MEDICAL_ASSISTANT\"]\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = MemorySaver()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, top_p=0.95)\n",
    "agent = Agent(model, [report_tool, organ_size_tool, chest_xray_tool], system=prompt, checkpointer=checkpointer)\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "Image(agent.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [HumanMessage(content=\"Does the image `demo/chest/pneumonia4.jpg` show pneumonia?\")]\n",
    "messages = [HumanMessage(content=\"Describe the radiology image `demo/chest/pneumonia4.jpg` in detail.\")]\n",
    "# messages = [HumanMessage(content=\"What is the volume of the heart (mL)?\")]\n",
    "# messages = [HumanMessage(content=\"What was the first question I asked?\")]\n",
    "# messages = [HumanMessage(content=\"What is the name and size of first organ mention in radiology report of `image.png`\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in agent.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "        kind = event[\"event\"]\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                print(content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "\n",
    "disable_torch_init()\n",
    "\n",
    "model_path = \"microsoft/llava-med-v1.5-mistral-7b\"\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is a chest X-ray, which is a common diagnostic imaging technique used to visualize the structures within the chest, including the lungs, heart, and bones of the chest and spine. In this particular image, there are no visible abnormalities, which means that the structures within the chest appear to be normal.\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"demo/chest/normal1.jpg\")\n",
    "question = \"Describe the image in detail.\"\n",
    "\n",
    "if model.config.mm_use_im_start_end:\n",
    "    question = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + question\n",
    "else:\n",
    "    question = DEFAULT_IMAGE_TOKEN + '\\n' + question\n",
    "\n",
    "conv = conv_templates[\"vicuna_v1\"].copy()\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "\n",
    "image_tensor = process_images([image], image_processor, model.config)[0]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor.unsqueeze(0).half().cuda(),\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=1000,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
